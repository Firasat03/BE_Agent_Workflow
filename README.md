# BE Multi-Agent Workflow

A **rule-based multi-agent pipeline** for backend code creation, updates, and modifications â€” powered by any LLM (Gemini, OpenAI, Anthropic, Ollama).

---

## Architecture

```
Task Prompt
     â†“
ğŸ›ï¸ Architect  â†’  ğŸ‘¤ Human Approval Gate
                         â†“
                   ğŸ’» Coder  â†’  ğŸ” Reviewer
                                      â†“
                               ğŸ§ª Tester â”€â”€FAILâ”€â”€â†’ ğŸ› Debugger â†’ ğŸ”„ retry
                                      â”‚
                                    PASS
                                      â†“
                               ğŸ“ Writer  â†’  âœ… Done
```

### Agents

| Agent | Role |
|-------|------|
| ğŸ›ï¸ **Architect** | Analyses the task + project; produces a structured implementation plan |
| ğŸ’» **Coder** | Generates complete source files from the plan (or applies Debugger fixes) |
| ğŸ” **Reviewer** | Code review â€” correctness, security, style, user-rule compliance |
| ğŸ§ª **Tester** | Writes pytest suites, flushes files to disk, runs tests |
| ğŸ› **Debugger** | Diagnoses failures, emits precise fix instructions for Coder |
| ğŸ“ **Writer** | Updates docstrings, README, CHANGELOG; optional git commit |

### Key Design Principles

- **Orchestrator is rule-based** â€” pure `if/while` logic, no LLM involved in routing
- **User rules injected into every agent** â€” define once in `rules/RULES.md`, enforced everywhere
- **Checkpointing after every agent** â€” crash-safe; resume with `--resume <run-id>`
- **MCP for external tools** â€” filesystem, knowledge base, Postgres, GitHub, SonarQube (pluggable)
- **LLM-agnostic** â€” switch providers via one env var, no code changes

---

## Quickstart

### 1. Install

```bash
cd be-agent-workflow
py -3 -m pip install -r requirements.txt
```

### 2. Set your API key

```bash
# Windows PowerShell
$env:GEMINI_API_KEY = "your-key-here"

# macOS / Linux
export GEMINI_API_KEY="your-key-here"
```

### 3. Run

```bash
# New task
py -3 main.py --task "Add POST /login endpoint" --project-root ./my_api

# With a specific coding rules profile
py -3 main.py --task "Add payment service" --rules rules/spring-boot.md --project-root ./billing

# Resume a crashed run
py -3 main.py --resume <run-id>

# List all past runs
py -3 main.py --list-runs
```

---

## Switching LLM Providers

Change one environment variable â€” no code changes needed:

```bash
# OpenAI
$env:LLM_PROVIDER = "openai"
$env:LLM_MODEL    = "gpt-4o"
$env:OPENAI_API_KEY = "sk-..."

# Anthropic Claude
$env:LLM_PROVIDER = "anthropic"
$env:LLM_MODEL    = "claude-3-5-sonnet-20241022"
$env:ANTHROPIC_API_KEY = "..."

# Ollama (local â€” free)
$env:LLM_PROVIDER = "ollama"
$env:LLM_MODEL    = "llama3.1"

# Any OpenAI-compatible API (Groq, Together, etc.)
$env:LLM_PROVIDER = "openai_compat"
$env:LLM_BASE_URL = "https://api.groq.com/openai/v1"
$env:LLM_MODEL    = "llama-3.1-70b-versatile"
$env:OPENAI_API_KEY = "gsk_..."
```

---

## User Coding Rules

Edit `rules/RULES.md` to define your team's standards.
Rules are **injected into every agent's system prompt** before each run.

```bash
# Use default rules
py -3 main.py --task "..."

# Use a specific profile
py -3 main.py --task "..." --rules rules/spring-boot.md
```

Available profiles: `rules/RULES.md` Â· `rules/spring-boot.md` Â· `rules/fastapi.md` Â· `rules/security-strict.md`

---

## Project Structure

```
be-agent-workflow/
â”œâ”€â”€ main.py                    â† CLI entry point
â”œâ”€â”€ orchestrator.py            â† Pipeline controller (no LLM)
â”œâ”€â”€ state.py                   â† PipelineState shared by all agents
â”œâ”€â”€ config.py                  â† All settings & env vars
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base_agent.py          â† Gemini/LLM call + rules injection (base class)
â”‚   â”œâ”€â”€ architect_agent.py
â”‚   â”œâ”€â”€ coder_agent.py
â”‚   â”œâ”€â”€ reviewer_agent.py
â”‚   â”œâ”€â”€ tester_agent.py
â”‚   â”œâ”€â”€ debugger_agent.py
â”‚   â””â”€â”€ writer_agent.py
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ llm_provider.py        â† Pluggable LLM factory
â”‚   â”œâ”€â”€ file_tools.py          â† read / write / list / tree
â”‚   â”œâ”€â”€ shell_tools.py         â† run commands, pytest runner
â”‚   â”œâ”€â”€ git_tools.py           â† diff, stage, commit
â”‚   â”œâ”€â”€ checkpoint_tools.py    â† crash-safe state persistence
â”‚   â”œâ”€â”€ rules_loader.py        â† load RULES.md
â”‚   â””â”€â”€ mcp_client.py          â† MCP client factory (per-agent access)
â”‚
â”œâ”€â”€ rules/                     â† Coding standards profiles
â”œâ”€â”€ mcp/agent_mcp_config.json  â† Per-agent MCP server permissions
â”œâ”€â”€ docs/                      â† Developer guides
â””â”€â”€ .workflow/                 â† Checkpoints (gitignored)
```

---

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `GEMINI_API_KEY` | *(required for gemini)* | Gemini API key |
| `OPENAI_API_KEY` | *(required for openai)* | OpenAI API key |
| `ANTHROPIC_API_KEY` | *(required for anthropic)* | Anthropic API key |
| `LLM_PROVIDER` | `gemini` | `gemini` \| `openai` \| `anthropic` \| `ollama` \| `openai_compat` |
| `LLM_MODEL` | `gemini-2.0-flash` | Model name for the chosen provider |
| `LLM_BASE_URL` | â€” | Base URL for `openai_compat` / Ollama |
| `LLM_TEMPERATURE` | `0.2` | Generation temperature |
| `LLM_MAX_TOKENS` | `8192` | Max output tokens |
| `MAX_DEBUG_RETRIES` | `3` | Max Debuggerâ†’Coder retry cycles |
| `MAX_REVIEW_RETRIES` | `1` | Max Reviewerâ†’Coder retry cycles |

---

## Docs

| Guide | Description |
|-------|-------------|
| [`docs/code_guide.md`](docs/code_guide.md) | Full code walkthrough â€” what every file does |
| [`docs/mcp_integration_guide.md`](docs/mcp_integration_guide.md) | How to connect real MCP servers & knowledge bases |
